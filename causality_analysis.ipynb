{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf193265",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def transfer_entropy_granger(source, target, lag):\n",
    "    T = len(source)\n",
    "    #Initially only some stores were tested with few data points so this formula made sense for stability however it's optional\n",
    "    bins = max(5, min(int(np.sqrt(T/5)), 20))\n",
    "    #Source is always in the past as noted by Granger (1980)\n",
    "    source = source[:-lag]\n",
    "    past = target[:-lag]\n",
    "    future = target[lag:]\n",
    "\n",
    "    min_val = min(source.min(), past.min(), future.min())\n",
    "    max_val = max(source.max(), past.max(), future.max())\n",
    "    #edges = np.linspace(min_val , max_val , bins + 1)\n",
    "    edges = np.linspace(min_val, max_val, bins + 1)\n",
    "    #This method follows the same logic as the rest, basically join information baseline vs augmented join information\n",
    "    #target future and past join probability\n",
    "    p_join_future_past,hist_f_p_x,hist_f_p_y = np.histogram2d(future,\n",
    "                                         past, bins=edges)\n",
    "    #Join probability including the source to target future and past\n",
    "    p_join_future_past_source,edges_hist = np.histogramdd(\n",
    "          np.column_stack([future, past, source]),\n",
    "        bins=[edges, edges, edges]\n",
    "    )\n",
    "    p_join_future_past_source = p_join_future_past_source/T\n",
    "    #Join probability including the source to target future and past\n",
    "    p_join_past_source, hist_p_s_x, hist_p_s_y = np.histogram2d(past, source,\n",
    "                                  bins=edges)\n",
    "    p_past, hist_p = np.histogram(past, bins=edges)\n",
    "\n",
    "    p_future_given_source_past = np.zeros((bins, bins, bins))\n",
    "    #find the probability of the future given the past already happened\n",
    "    p_future_given_past = np.zeros((bins, bins))\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            if p_past[j] > 1e-10:\n",
    "                p_future_given_past[i, j] = p_join_future_past[i, j] / p_past[j]\n",
    "            else:\n",
    "                p_future_given_past[i, j] = 1e-10\n",
    "    #Find the probability of the target future given that variable source and past happened\n",
    "    p_future_given_both = np.zeros((bins, bins, bins))\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            for k in range(bins):\n",
    "                if p_join_past_source[j, k] > 1e-10:\n",
    "                    p_future_given_both[i, j, k] = p_join_future_past_source[i, j, k] / p_join_past_source[j, k]\n",
    "                else:\n",
    "                    p_future_given_both[i, j, k] = 1e-10\n",
    "    #Conditional entropy of future given past only \n",
    "    entropy_past_to_future = 0.0\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            if p_future_given_past[i, j] > 1e-10:\n",
    "                entropy_past_to_future -= p_future_given_past[i, j] * np.log2(p_future_given_past[i, j])\n",
    "    #Conditional entropy of the future given the past happened and adding the tested source\n",
    "    entropy_source_past_to_future = 0.0\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            for k in range(bins):\n",
    "                if p_future_given_source_past[i, j, k] > 1e-10:\n",
    "                    entropy_source_past_to_future -= p_future_given_source_past[i, j, k] * np.log2(p_future_given_source_past[i, j, k])\n",
    "    #Reduction in uncertainty, gives the information gain \n",
    "    transfer_entropy = entropy_past_to_future - entropy_source_past_to_future\n",
    "    return max(0, transfer_entropy/len(future))\n",
    "# This function had fix variables for reproducibility against timegraph but they might be changed for improvements\n",
    "# main instrument for improving causality is actually threshold so basic parameters work fine\n",
    "def entropy_enhanced_random_forest(\n",
    "    df_path\n",
    "    ,max_lag=2\n",
    "    ,n_estimators=100\n",
    "    ,max_depth=10\n",
    "    ,min_samples_split=5\n",
    "    ,min_samples_leaf=2\n",
    "    ,max_features='sqrt'\n",
    "    ,random_state=42\n",
    "):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(df_path)\n",
    "    #Initially jsut removing time work for tests, but confounders are not removed by this\n",
    "    #df = df.drop('time', axis=1)\n",
    "\n",
    "    # this is where changes need to be manually make, this works for time graph, other test require different handling\n",
    "    variables = [col for col in df.columns if col.startswith('X')]\n",
    "    n_vars = len(variables)\n",
    "    #one matrix for importances, for information weight and one to assign lags\n",
    "    random_forest_importance_matrix = np.zeros((n_vars, n_vars))\n",
    "    lag_matrix_importance_entropy_transfer = np.zeros((n_vars, n_vars, max_lag))\n",
    "    entropy = np.zeros((n_vars, n_vars, max_lag))\n",
    "    for source_index, source in enumerate(variables):\n",
    "        for target_index, target in enumerate(variables):\n",
    "            if source_index != target_index:\n",
    "                for lag in range(1, max_lag + 1):\n",
    "                    te = transfer_entropy_granger( df[source].values,\n",
    "                        df[target].values,\n",
    "                        lag)\n",
    "                    entropy[source_index, target_index, lag-1] = te\n",
    "\n",
    "    lagged_data = {}\n",
    "    for var in variables:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            lagged_data[f'{var}_lag{lag}'] = df[var].shift(lag)\n",
    "\n",
    "    df_full = pd.concat([df, pd.DataFrame(lagged_data)], axis=1)\n",
    "    df_full = df_full.dropna()\n",
    "\n",
    "\n",
    "    combined_importance_matrix = np.zeros((n_vars, n_vars))\n",
    "    for target_index, target in enumerate(variables):\n",
    "        features = []\n",
    "        pred_lags = {}\n",
    "\n",
    "        for source_index, source in enumerate(variables):\n",
    "            if source_index != target_index:\n",
    "                for lag in range(1, max_lag + 1):\n",
    "                    col_name = f'{source}_lag{lag}'\n",
    "                    features.append(col_name)\n",
    "                    pred_lags[col_name] = (source_index, lag-1)\n",
    "\n",
    "        X = df_full[features].values\n",
    "        y = df_full[target].values\n",
    "        random_forest = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        random_forest.fit(X, y)\n",
    "        random_forest_importance = random_forest.feature_importances_\n",
    "\n",
    "        for index, col in enumerate(features):\n",
    "            source_index, lag_index = pred_lags[col]\n",
    "            base = random_forest_importance[index]\n",
    "            transfer_entropy_value = entropy[source_index, target_index, lag_index]\n",
    "            transfer_entropy_normalized = transfer_entropy_value / (np.max(entropy)+0.0000001)\n",
    "            # transfer entropy increased the weights for the random_forest, so a reduced addition made sense\n",
    "            combined_importance_information = (1 - 0.3) * base + 0.3 * transfer_entropy_normalized\n",
    "\n",
    "            lag_matrix_importance_entropy_transfer[source_index, target_index, lag_index] = combined_importance_information\n",
    "            random_forest_importance_matrix[source_index, target_index] += base\n",
    "            combined_importance_matrix[source_index, target_index] += combined_importance_information\n",
    "   #it made more sense to have different thresholds taking both statistical methods into account than the most common used\n",
    "    flat_scores = [combined_importance_matrix[i, j] for i in range(n_vars) for j in range(n_vars) if i != j]\n",
    "\n",
    "   #lower tresholds leads to wosrt linear results\n",
    "    sorted_scores = sorted(flat_scores, reverse=True)\n",
    "    threshold_index = n_vars*2 + 1\n",
    "    threshold = sorted_scores[threshold_index] if len(sorted_scores) > threshold_index else 0.0\n",
    "    random_forest_threshold = np.percentile(random_forest_importance_matrix[random_forest_importance_matrix > 0], 70)\n",
    "    max_transfer_values = np.max(entropy, axis=2)\n",
    "    transfer_entropy_max = max_transfer_values[max_transfer_values > 0]\n",
    "\n",
    "    #transfer entropy tends to be to permissive or too restrictive, 50% made sense after some tests or 0 if too restrictive\n",
    "    if len(transfer_entropy_max) > 0:\n",
    "        entropy_threshold = np.percentile(transfer_entropy_max, 50)\n",
    "    else:\n",
    "        entropy_threshold = 0\n",
    "\n",
    "    causality_matrix = np.zeros((n_vars, n_vars))\n",
    "    predicted_lags = np.zeros((n_vars, n_vars))\n",
    "    print(combined_importance_matrix)\n",
    "    for i in range(n_vars):\n",
    "        for j in range(n_vars):\n",
    "            if i != j:\n",
    "                random_forest_score = random_forest_importance_matrix[i, j]\n",
    "                max_transfer_entropy = np.max(entropy[i, j, :])\n",
    "\n",
    "                if entropy_threshold > 0:\n",
    "                    if (max_transfer_entropy > entropy_threshold or random_forest_score > random_forest_threshold):\n",
    "                        if combined_importance_matrix[i,j] > threshold:\n",
    "                            causality_matrix[i, j] = 1\n",
    "                else:\n",
    "                    if random_forest_score > random_forest_threshold:\n",
    "                        causality_matrix[i, j] = 1\n",
    "\n",
    "                if causality_matrix[i, j] == 1:\n",
    "                    best_lag_index = np.argmax(lag_matrix_importance_entropy_transfer[i, j, :])\n",
    "                    predicted_lags[i, j] = best_lag_index + 1\n",
    "\n",
    "    return causality_matrix.astype(int), predicted_lags.astype(int)\n",
    "def prepare_causality_data(df, price_col='amazon_price_unified'):\n",
    "\n",
    "\n",
    "    price_metrics = [\n",
    "        f'{price_col}_frequency',\n",
    "        f'{price_col}_realized_variance',\n",
    "        f'{price_col}_intraday_range'\n",
    "    ]\n",
    "\n",
    "\n",
    "    other_variables = ['store_rank', 'reviews']\n",
    "    if df['store_rank'].sum() < 0:\n",
    "        other_variables = ['reviews']\n",
    "    all_variables = price_metrics + other_variables\n",
    "\n",
    "    prepared_data = {}\n",
    "\n",
    "    for category in df['category'].unique():\n",
    "        category_df = df[df['category'] == category].copy()\n",
    "\n",
    "        category_data = {}\n",
    "\n",
    "        for variable in all_variables:\n",
    "            if variable not in category_df.columns:\n",
    "                continue\n",
    "\n",
    "            variable_df = category_df[['date', 'query', variable]].copy()\n",
    "\n",
    "            pivot_df = variable_df.pivot_table(\n",
    "                index='date',\n",
    "                columns='query',\n",
    "                values=variable,\n",
    "                aggfunc='first'\n",
    "            )\n",
    "\n",
    "            pivot_df.columns = [f\"{query}_{variable}\" for query in pivot_df.columns]\n",
    "\n",
    "            category_data[variable] = pivot_df\n",
    "\n",
    "        prepared_data[category] = category_data\n",
    "\n",
    "    return prepared_data\n",
    "df_full = pd.read_csv('/content/drive/MyDrive/transformed_data/amazon_pricing.csv')\n",
    "df_cat = pd.read_csv('/content/drive/MyDrive/keepa_data/final_df.csv')\n",
    "df_cat = df_cat[['query','category']].drop_duplicates()\n",
    "df_analysis = pd.merge(df_full,df_cat,how='left',on=['query'])\n",
    "df_analysis['date'] = pd.to_datetime(df_analysis['date'])\n",
    "df_analysis = df_analysis[((df_analysis['date'] >= '2024-09-01') &\n",
    "        (df_analysis['date'] < '2025-01-01'))]\n",
    "price_col='amazon_price'\n",
    "prepared_data = prepare_causality_data(df_analysis, price_col='amazon_price')\n",
    "metrics = {\n",
    "    'frequency': f'{price_col}_frequency',\n",
    "    'variance': f'{price_col}_realized_variance',\n",
    "    'range': f'{price_col}_intraday_range'\n",
    "}\n",
    "\n",
    "max_lag=7\n",
    "\n",
    "results = []\n",
    "\n",
    "for category, category_data in prepared_data.items():\n",
    "    # Get unique queries in this category\n",
    "    # Query is the name of the product given the extraction of the data from keepa\n",
    "    # Query = Amazon Product ASIN Code\n",
    "    queries = set()\n",
    "    for df in category_data.values():\n",
    "        # Extract all the products or \"queries\"\n",
    "        queries.update([col.rsplit('_', len(col.split('_')) - 1)[0] for col in df.columns])\n",
    "    for query in queries:\n",
    "        query_results = {\n",
    "            'query': query,\n",
    "            'category': category\n",
    "        }\n",
    "\n",
    "        # All values are assigned as 0 unless causality is found\n",
    "        for metric_name in metrics.keys():\n",
    "            query_results[f'competitors_cause_{metric_name}'] = 0\n",
    "            query_results[f'competitors_lag_{metric_name}'] = 0\n",
    "\n",
    "            query_results[f'rank_causes_{metric_name}'] = 0\n",
    "            query_results[f'rank_lag_{metric_name}'] = 0\n",
    "\n",
    "            query_results[f'reviews_cause_{metric_name}'] = 0\n",
    "            query_results[f'reviews_lag_{metric_name}'] = 0\n",
    "\n",
    "        # For this research only metric specific causality was calculated\n",
    "        # hence the causality is done through metric specific loops\n",
    "        for metric_name, metric_var in metrics.items():\n",
    "            metric_df = category_data[metric_var]\n",
    "            target_col = f\"{query}_{metric_var}\"\n",
    "\n",
    "            combined_df_list = []\n",
    "            col_mapping = {}\n",
    "            col_idx = 0\n",
    "\n",
    "            # Add target column (query's metric)\n",
    "            combined_df_list.append(metric_df[[target_col]])\n",
    "            col_mapping[col_idx] = ('target', metric_var, target_col, query)\n",
    "            col_idx += 1\n",
    "\n",
    "            # Add source columns (same metric from competitors)\n",
    "            for col in metric_df.columns:\n",
    "                col_query = col.rsplit('_', len(col.split('_')) - 1)[0]\n",
    "                if col_query != query:  # Only competitors\n",
    "                    combined_df_list.append(metric_df[[col]])\n",
    "                    col_mapping[col_idx] = ('competitor', metric_var, col, col_query)\n",
    "                    col_idx += 1\n",
    "\n",
    "            # Add rank for the query\n",
    "            if 'store_rank' in category_data:\n",
    "                var_df = category_data['store_rank']\n",
    "                var_col = f\"{query}_store_rank\"\n",
    "                if var_col in var_df.columns:\n",
    "                    combined_df_list.append(var_df[[var_col]])\n",
    "                    col_mapping[col_idx] = ('rank', 'store_rank', var_col, query)\n",
    "                    col_idx += 1\n",
    "\n",
    "            # Add reviews for the query\n",
    "            if 'reviews' in category_data:\n",
    "                var_df = category_data['reviews']\n",
    "                var_col = f\"{query}_reviews\"\n",
    "                if var_col in var_df.columns:\n",
    "                    combined_df_list.append(var_df[[var_col]])\n",
    "                    col_mapping[col_idx] = ('reviews', 'reviews', var_col, query)\n",
    "                    col_idx += 1\n",
    "\n",
    "            combined_df = pd.concat(combined_df_list, axis=1).dropna()\n",
    "\n",
    "\n",
    "            # Run causality analysis\n",
    "            causality_matrix, lag_matrix = entropy_enhanced_random_forest(combined_df, max_lag)\n",
    "\n",
    "            # Process results for this metric\n",
    "            competitor_lags = []\n",
    "            num_competitor_causal = 0\n",
    "\n",
    "            source_indices = [i for i, info in col_mapping.items() if info[0] != 'target']\n",
    "            target_idx = 0  # Target is always first\n",
    "\n",
    "            for source_idx in source_indices:\n",
    "                if source_idx < causality_matrix.shape[0] and target_idx < causality_matrix.shape[1]:\n",
    "                    if causality_matrix[source_idx, target_idx] == 1:\n",
    "                        source_info = col_mapping[source_idx]\n",
    "                        source_type = source_info[0]\n",
    "                        lag_value = lag_matrix[source_idx, target_idx]\n",
    "\n",
    "                        # Handle different source types\n",
    "                        if source_type == 'competitor':\n",
    "                            num_competitor_causal += 1\n",
    "                            competitor_lags.append(lag_value)\n",
    "\n",
    "                        elif source_type == 'rank':\n",
    "                            query_results[f'rank_causes_{metric_name}'] = 1\n",
    "                            query_results[f'rank_lag_{metric_name}'] = lag_value\n",
    "\n",
    "                        elif source_type == 'reviews':\n",
    "                            query_results[f'reviews_cause_{metric_name}'] = 1\n",
    "                            query_results[f'reviews_lag_{metric_name}'] = lag_value\n",
    "\n",
    "            query_results[f'competitors_cause_{metric_name}'] = num_competitor_causal\n",
    "            #If inside the category there are no competitors then, nothing is calculated\n",
    "            if competitor_lags:\n",
    "                query_results[f'competitors_lag_{metric_name}'] = sum(competitor_lags) / len(competitor_lags)\n",
    "            else:\n",
    "                query_results[f'competitors_lag_{metric_name}'] = 0\n",
    "\n",
    "        results.append(query_results)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "base_cols = ['query', 'category']\n",
    "metric_cols = []\n",
    "\n",
    "for metric_name in ['frequency', 'variance', 'range']:\n",
    "    metric_cols.extend([\n",
    "        f'competitors_cause_{metric_name}',\n",
    "        f'competitors_lag_{metric_name}',\n",
    "        f'rank_causes_{metric_name}',\n",
    "        f'rank_lag_{metric_name}',\n",
    "        f'reviews_cause_{metric_name}',\n",
    "        f'reviews_lag_{metric_name}'\n",
    "    ])\n",
    "\n",
    "\n",
    "all_cols = base_cols + metric_cols\n",
    "existing_columns = [col for col in all_cols if col in results_df.columns]\n",
    "results_df = results_df[existing_columns]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
